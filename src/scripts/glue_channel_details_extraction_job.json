{
	"jobConfig": {
		"name": "glue_channel_details_extraction_job",
		"description": "Channel Details Extraction Job",
		"role": "arn:aws:iam::014498657303:role/awsglue_role1",
		"command": "glueetl",
		"version": "4.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"maxRetries": 0,
		"timeout": 2880,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "glue_channel_details_extraction_job.py",
		"scriptLocation": "s3://aws-glue-assets-014498657303-us-east-1/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2024-08-08T12:57:34.253Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-014498657303-us-east-1/temporary/",
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": false,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-014498657303-us-east-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import json\r\nimport requests\r\nimport pandas as pd\r\nimport boto3\r\nfrom io import StringIO\r\nfrom pyspark.context import SparkContext\r\nfrom pyspark.sql import SparkSession\r\n\r\n# Initialize Spark Context and Session\r\nsc = SparkContext()\r\nspark = SparkSession(sc)\r\n\r\n# S3 Client\r\ns3_client = boto3.client('s3')\r\n\r\n# Function to read data from S3\r\ndef read_from_s3(bucket, key):\r\n    response = s3_client.get_object(Bucket=bucket, Key=key)\r\n    data = response['Body'].read().decode('utf-8')\r\n    return data\r\n\r\n# Read API keys from S3\r\ndef read_api_keys_from_s3(bucket, key):\r\n    data = read_from_s3(bucket, key)\r\n    api_keys = json.loads(data)\r\n    return api_keys['VIDEO_API_KEYS']\r\n\r\n# Read channel IDs from S3\r\ndef read_channel_ids_from_s3(bucket, key):\r\n    data = read_from_s3(bucket, key)\r\n    df = pd.read_csv(StringIO(data))\r\n    return df\r\n\r\n# Fetch channel details\r\ndef fetch_channel_details(api_key, channel_id):\r\n    url = f'https://www.googleapis.com/youtube/v3/channels?part=statistics,snippet&id={channel_id}&key={api_key}'\r\n    response = requests.get(url)\r\n    data = response.json()\r\n    if 'items' in data and data['items']:\r\n        item = data['items'][0]\r\n        return {\r\n            'Channel ID': item['id'],\r\n            'Channel Name': item['snippet']['title'],\r\n            'Subscriber Count': item['statistics'].get('subscriberCount', 0),\r\n            'Video Count': item['statistics'].get('videoCount', 0)\r\n        }\r\n    return None\r\n\r\n# S3 details\r\nS3_BUCKET = 'sphtest512'\r\nAPI_KEYS_S3_KEY = 'data/API_keys.json'\r\nCHANNEL_IDS_S3_KEY = 'data/channel_ids.csv'\r\nRESULTS_S3_KEY = 'data/channel_details/channel_details.csv'\r\n\r\n# Fetch API keys and channel IDs\r\napi_keys = read_api_keys_from_s3(S3_BUCKET, API_KEYS_S3_KEY)\r\nchannel_ids_df = read_channel_ids_from_s3(S3_BUCKET, CHANNEL_IDS_S3_KEY)\r\n\r\n# Fetch channel details using the first API key (for simplicity)\r\napi_key = api_keys[0]\r\nresults = []\r\nfor _, row in channel_ids_df.iterrows():\r\n    channel_id = row['Channel ID']\r\n    details = fetch_channel_details(api_key, channel_id)\r\n    if details:\r\n        results.append(details)\r\n\r\n# Convert results to DataFrame and CSV\r\nresults_df = pd.DataFrame(results)\r\ncsv_buffer = StringIO()\r\nresults_df.to_csv(csv_buffer, index=False)\r\n\r\n# Upload results to S3\r\ns3_client.put_object(Bucket=S3_BUCKET, Key=RESULTS_S3_KEY, Body=csv_buffer.getvalue())\r\n\r\nprint(f'Successfully uploaded channel details CSV to S3 bucket \"{S3_BUCKET}\" at \"{RESULTS_S3_KEY}\"')\r\n"
}