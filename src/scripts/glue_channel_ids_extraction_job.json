{
	"jobConfig": {
		"name": "glue_channel_ids_extraction_job",
		"description": "Channel IDs Extraction Job",
		"role": "arn:aws:iam::014498657303:role/awsglue_role1",
		"command": "glueetl",
		"version": "4.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"maxRetries": 0,
		"timeout": 2880,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "glue_channel_ids_extraction_job.py",
		"scriptLocation": "s3://aws-glue-assets-014498657303-us-east-1/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2024-08-08T12:54:27.222Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-014498657303-us-east-1/temporary/",
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": false,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-014498657303-us-east-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import requests\r\nfrom pyspark.context import SparkContext\r\nfrom pyspark.sql import SparkSession\r\nfrom pyspark.sql.functions import lit\r\nimport boto3\r\nfrom io import StringIO\r\nimport pandas as pd\r\n\r\ndef fetch_channel_id(api_key, handle):\r\n    url = f'https://www.googleapis.com/youtube/v3/search?part=snippet&type=channel&q={handle}&key={api_key}'\r\n    response = requests.get(url)\r\n    data = response.json()\r\n    if 'items' in data and data['items']:\r\n        return data['items'][0]['id']['channelId']\r\n    return None\r\n\r\n# Initialize Spark Context and Session\r\nsc = SparkContext()\r\nspark = SparkSession(sc)\r\n\r\n# List of custom handles\r\nCUSTOM_HANDLES = [\r\n    'straitstimesonline',\r\n    'TheBusinessTimes',\r\n    'zaobaosg',\r\n    'Tamil_Murasu',\r\n    'BeritaHarianSG1957'\r\n]\r\n\r\nAPI_KEY = 'AIzaSyD9wyRmGCOihj5_r46cD3mrhX2UmrjrnFk'\r\nS3_BUCKET = 'sphtest512'\r\nS3_KEY = 'data/channel_details/channel_ids.csv'\r\n\r\n# Create a Spark DataFrame\r\ndef fetch_data():\r\n    data = [(handle, fetch_channel_id(API_KEY, handle)) for handle in CUSTOM_HANDLES]\r\n    df = spark.createDataFrame(data, [\"Handle\", \"Channel ID\"])\r\n    return df\r\n\r\ndf = fetch_data()\r\n\r\n# Convert DataFrame to Pandas DataFrame for CSV conversion\r\npandas_df = df.toPandas()\r\ncsv_buffer = StringIO()\r\npandas_df.to_csv(csv_buffer, index=False)\r\n\r\n# Upload CSV to S3\r\ns3_client = boto3.client('s3')\r\ns3_client.put_object(Bucket=S3_BUCKET, Key=S3_KEY, Body=csv_buffer.getvalue())\r\n\r\nprint(f'Successfully uploaded CSV to S3 bucket \"{S3_BUCKET}\" at \"{S3_KEY}\"')\r\n"
}