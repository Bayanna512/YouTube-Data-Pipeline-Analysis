{
	"jobConfig": {
		"name": "glue_popular_video_details_extraction_job",
		"description": "Popular Video Details Extraction Job",
		"role": "arn:aws:iam::014498657303:role/awsglue_role1",
		"command": "glueetl",
		"version": "4.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"maxRetries": 0,
		"timeout": 2880,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "glue_popular_video_details_extraction_job.py",
		"scriptLocation": "s3://aws-glue-assets-014498657303-us-east-1/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2024-08-08T12:59:30.171Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-014498657303-us-east-1/temporary/",
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": false,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-014498657303-us-east-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.transforms import *\r\nfrom awsglue.utils import getResolvedOptions\r\nimport requests\r\nimport json\r\nimport boto3\r\n\r\n# Initialize Glue context\r\nglueContext = GlueContext(SparkContext.getOrCreate())\r\nspark = glueContext.spark_session\r\n\r\n# S3 bucket details\r\ns3_bucket = 'sphtest512'\r\nchannel_details_path = f's3://{s3_bucket}/data/channel_details.csv'\r\napi_keys_path = f's3://{s3_bucket}/data/API_keys.json'\r\n\r\n# Read channel details from S3\r\nchannel_details_df = spark.read.csv(channel_details_path, header=True)\r\n\r\n# Debugging: Print the schema and data of the DataFrame\r\nchannel_details_df.printSchema()\r\nchannel_details_df.show(5)\r\n\r\n# Extract channel IDs\r\nchannel_ids = [row['Channel ID'] for row in channel_details_df.collect()]\r\n\r\n# Read API keys from S3\r\ns3_client = boto3.client('s3')\r\napi_keys_object = s3_client.get_object(Bucket=s3_bucket, Key='data/API_keys.json')\r\napi_keys_content = api_keys_object['Body'].read().decode('utf-8')\r\napi_keys = json.loads(api_keys_content)['VIDEO_API_KEYS']\r\n\r\n# Function to fetch top 50 videos for a given channel ID\r\ndef fetch_top_videos(channel_id, api_key):\r\n    YOUTUBE_API_SEARCH_URL = 'https://www.googleapis.com/youtube/v3/search'\r\n    YOUTUBE_API_VIDEOS_URL = 'https://www.googleapis.com/youtube/v3/videos'\r\n    \r\n    # Fetching video IDs\r\n    search_params = {\r\n        'part': 'snippet',\r\n        'channelId': channel_id,\r\n        'maxResults': 50,\r\n        'order': 'viewCount',\r\n        'type': 'video',\r\n        'key': api_key\r\n    }\r\n    search_response = requests.get(YOUTUBE_API_SEARCH_URL, params=search_params)\r\n    \r\n    if search_response.status_code == 200:\r\n        videos = search_response.json().get('items', [])\r\n        video_ids = [video['id']['videoId'] for video in videos]\r\n        \r\n        # Fetching video statistics and publish date\r\n        videos_params = {\r\n            'part': 'statistics,snippet',\r\n            'id': ','.join(video_ids),\r\n            'key': api_key\r\n        }\r\n        videos_response = requests.get(YOUTUBE_API_VIDEOS_URL, params=videos_params)\r\n        \r\n        if videos_response.status_code == 200:\r\n            video_statistics = videos_response.json().get('items', [])\r\n            return [\r\n                (\r\n                    channel_id,\r\n                    video['id'],\r\n                    next(v['snippet']['title'] for v in videos if v['id']['videoId'] == video['id']),\r\n                    int(video['statistics'].get('viewCount', 0)),\r\n                    video['snippet']['publishedAt']\r\n                ) for video in video_statistics\r\n            ]\r\n        else:\r\n            videos_response.raise_for_status()\r\n    else:\r\n        search_response.raise_for_status()\r\n\r\n# Initialize a list to store video data\r\nvideo_data = []\r\n\r\n# Loop through each channel ID and fetch the top 50 videos\r\nfor api_key in api_keys:\r\n    for channel_id in channel_ids:\r\n        video_data.extend(fetch_top_videos(channel_id, api_key))\r\n\r\n# Create DataFrame\r\ncolumns = ['ChannelID', 'VideoID', 'Title', 'ViewCount', 'PublishDate']\r\ndf = spark.createDataFrame(video_data, columns)\r\n\r\n# Show the top 50 videos\r\ndf.show(50, truncate=False)\r\n\r\n# Save the DataFrame to S3 as a single CSV file\r\noutput_path = f's3://{s3_bucket}/data/popular_video_details/'\r\ndf.coalesce(1).write.mode('overwrite').csv(output_path, header=True)\r\n\r\n# Stop the Spark session\r\nspark.stop()\r\n"
}